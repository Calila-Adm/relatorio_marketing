"""
M√≥dulo principal da automa√ß√£o de relat√≥rios mensais do I-Club.

Este m√≥dulo orquestra todo o processo de extra√ß√£o de dados do programa de fidelidade I-Club,
gerando relat√≥rios mensais em Excel.

Funcionalidades principais:
- Extra√ß√£o de dados do PostgreSQL usando queries complexas
- Gera√ß√£o de arquivo Excel com m√∫ltiplas abas de an√°lise
- Tratamento robusto de erros e logging detalhado

Autor: Marketing Team - Iguatemi
Data: 2025
Vers√£o: 2.0
"""

import os
import pandas as pd
import logging
from datetime import datetime
from dateutil.relativedelta import relativedelta
from sqlalchemy import create_engine, text
from sqlalchemy.pool import QueuePool
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from dotenv import load_dotenv
from queries import QUERIES

# M√≥dulos removidos: email_service e email_formatter (funcionalidade de email removida)

# Configura√ß√£o b√°sica de logging
# O sistema mant√©m logs tanto em arquivo quanto no console para facilitar monitoramento
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("automation.log"),  # Arquivo de log persistente
        logging.StreamHandler()  # Output no console para execu√ß√£o manual
    ]
)

def create_performance_indexes(engine):
    """
    Cria √≠ndices de performance para otimizar as queries do I-Club
    
    Esta fun√ß√£o cria √≠ndices estrat√©gicos nas tabelas mais utilizadas
    para melhorar significativamente a performance das consultas.
    """
    indexes_sql = [
        # √çndices para tabela de transa√ß√µes/vendas
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_transacao_data ON transacao(data_transacao);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_transacao_cliente ON transacao(cliente_id);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_transacao_loja ON transacao(loja_id);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_transacao_status ON transacao(status);",
        
        # √çndices para tabela de cupons
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_cupom_data_emissao ON cupom(data_emissao);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_cupom_data_uso ON cupom(data_uso);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_cupom_cliente ON cupom(cliente_id);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_cupom_status ON cupom(status);",
        
        # √çndices para tabela de clientes
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_cliente_categoria ON cliente(categoria);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_cliente_data_cadastro ON cliente(data_cadastro);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_cliente_ativo ON cliente(ativo);",
        
        # √çndices para tabela de visitas
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_visita_data ON visita(data_visita);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_visita_cliente ON visita(cliente_id);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_visita_loja ON visita(loja_id);",
        
        # √çndices compostos para queries YoY (Year over Year)
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_transacao_data_cliente ON transacao(data_transacao, cliente_id);",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_cupom_data_cliente ON cupom(data_emissao, cliente_id);",
        
        # √çndices para performance de agrega√ß√µes
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_transacao_valor ON transacao(valor) WHERE valor > 0;",
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_transacao_mes_ano ON transacao(EXTRACT(YEAR FROM data_transacao), EXTRACT(MONTH FROM data_transacao));",
    ]
    
    try:
        logging.info("üîß Verificando/criando √≠ndices de performance...")
        
        with engine.connect() as conn:
            created_count = 0
            for sql in indexes_sql:
                try:
                    # Usar autocommit para CREATE INDEX CONCURRENTLY
                    conn.execute(text("COMMIT;"))  # Finalizar transa√ß√£o atual
                    conn.execute(text(sql))
                    created_count += 1
                except Exception as e:
                    # √çndice provavelmente j√° existe ou erro de sintaxe
                    if "already exists" not in str(e).lower():
                        logging.debug(f"Aviso ao criar √≠ndice: {e}")
            
            logging.info(f"‚úÖ √çndices de performance verificados/criados: {created_count}/{len(indexes_sql)}")
            
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel criar alguns √≠ndices: {e}")
        logging.info("Sistema continuar√° funcionando, mas com performance reduzida")


# Fun√ß√£o de email removida - sistema apenas gera arquivos Excel

def run_report_job():
    """
    Fun√ß√£o principal que orquestra todo o processo de gera√ß√£o de relat√≥rios do I-Club.
    
    Esta fun√ß√£o coordena todas as etapas da automa√ß√£o:
    1. Configura√ß√£o do ambiente e conex√£o com banco de dados
    2. Execu√ß√£o das queries SQL para extra√ß√£o de dados
    3. Gera√ß√£o do arquivo Excel com m√∫ltiplas abas
    
    Fluxo de Execu√ß√£o:
        1. Carrega vari√°veis de ambiente do arquivo .env
        2. Determina o per√≠odo do relat√≥rio (sempre m√™s anterior)
        3. Estabelece conex√£o com PostgreSQL
        4. Executa cada query do m√≥dulo QUERIES
        5. Salva resultados em arquivo Excel
        
    Tratamento de Erros:
        - Fase 1: Erros de configura√ß√£o (vari√°veis de ambiente ausentes)
        - Fase 2: Erros de execu√ß√£o de queries SQL
        - Fase 3: Erros de gera√ß√£o do Excel
        
    Tempo de Execu√ß√£o:
        - M√©dio: 3-4 minutos para processar todas as 12 queries
        - Depende do volume de dados e performance do banco
        
    Returns:
        None - A fun√ß√£o apenas executa e registra logs
    """
    logging.info("Iniciando processo de extra√ß√£o de relat√≥rios.")
    
    # Carrega vari√°veis de ambiente do arquivo .env
    load_dotenv()
    
    # Determina o m√™s do relat√≥rio (sempre m√™s anterior ao atual)
    # Ex: Se executado em julho/2025, gera relat√≥rio de junho/2025
    report_date = datetime.today() - relativedelta(months=1)
    report_month_str = report_date.strftime('%Y-%m')

    # --- FASE 1: Prepara√ß√£o do Ambiente e Conex√£o com Banco ---
    try:
        # Carrega credenciais do banco de dados PostgreSQL
        db_user = os.getenv("DB_USER")
        db_password = os.getenv("DB_PASSWORD")
        db_host = os.getenv("DB_HOST")
        db_port = os.getenv("DB_PORT")
        db_name = os.getenv("DB_NAME")
        # Criar pasta espec√≠fica para o m√™s anterior no Windows
        base_path = "/mnt/c/Users/edgar.prado/Documents/relatorio_fechamento_mensal"
        
        # Formata√ß√£o do nome da pasta: nomemes'YY (ex: janeiro'25, dezembro'24)
        meses_nomes = {
            1: "janeiro", 2: "fevereiro", 3: "mar√ßo", 4: "abril",
            5: "maio", 6: "junho", 7: "julho", 8: "agosto", 
            9: "setembro", 10: "outubro", 11: "novembro", 12: "dezembro"
        }
        
        month_name = meses_nomes[report_date.month]
        year_short = report_date.strftime('%y')  # Ano com 2 d√≠gitos
        folder_name = f"{month_name}'{year_short}"
        
        export_path = os.path.join(base_path, folder_name)

        # Valida√ß√£o de configura√ß√£o - vari√°veis de banco s√£o obrigat√≥rias
        if not all([db_user, db_password, db_host, db_port, db_name]):
            raise ValueError("Uma ou mais vari√°veis de ambiente do banco de dados n√£o foram definidas no arquivo .env.")
        
        # Criar pasta espec√≠fica para o m√™s (sempre cria, mesmo se j√° existir)
        logging.info(f"Criando pasta para o m√™s: {folder_name}")
        logging.info(f"Caminho completo: {export_path}")
        
        try:
            os.makedirs(export_path, exist_ok=True)
            logging.info(f"‚úÖ Pasta criada/verificada com sucesso: {export_path}")
        except Exception as e:
            logging.error(f"‚ùå N√£o foi poss√≠vel criar a pasta: {e}")
            raise ValueError(f"Pasta de destino inacess√≠vel: {export_path}")

        # Monta string de conex√£o PostgreSQL com connection pooling otimizado
        DATABASE_URL = f"postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
        engine = create_engine(
            DATABASE_URL,
            poolclass=QueuePool,
            pool_size=10,        # N√∫mero de conex√µes mantidas no pool
            max_overflow=20,     # Conex√µes adicionais se necess√°rio  
            pool_pre_ping=True,  # Verifica conex√µes antes de usar
            pool_recycle=3600,   # Recria conex√µes a cada hora
            echo=False           # Desabilita log SQL para performance
        )
        
        # Criar √≠ndices de performance se n√£o existirem
        create_performance_indexes(engine)
        
        # Define nome do arquivo de sa√≠da com padr√£o YYYY-MM
        file_name = f"Relatorio_Mensal_{report_month_str}.xlsx"
        full_file_path = os.path.join(export_path, file_name)

    except Exception as e:
        logging.error(f"Erro na configura√ß√£o inicial: {e}")
        logging.error("Script falhou durante a fase de configura√ß√£o inicial.")
        return

    # --- FASE 2: Execu√ß√£o Paralela de Queries e Gera√ß√£o de Arquivos Individuais ---
    # Lista para thread-safe status reporting
    status_report = []
    status_lock = threading.Lock()
    # Dicion√°rio para armazenar DataFrames
    dataframes_dict = {}
    dataframes_lock = threading.Lock()
    
    def execute_query_and_save(query_info):
        """Executa uma query e salva em arquivo individual"""
        name, query = query_info
        thread_name = threading.current_thread().name
        
        try:
            logging.info(f"[{thread_name}] Executando query: '{name}'...")
            
            # Criar conex√£o individual para thread safety
            with engine.connect() as conn:
                # Executa query SQL e converte resultado para DataFrame
                df = pd.read_sql_query(text(query), conn)
                
                # Thread-safe storage
                with dataframes_lock:
                    dataframes_dict[name] = df
                
                # Criar nome de arquivo individual (limpar caracteres especiais)
                safe_name = name.replace(" ", "_").replace("/", "-").replace(":", "-")
                safe_name = safe_name.replace("?", "").replace("*", "").replace("<", "").replace(">", "")
                safe_name = safe_name.replace("|", "-").replace('"', "").replace("'", "")
                
                individual_file_name = f"{safe_name}_{report_month_str}.xlsx"
                individual_file_path = os.path.join(export_path, individual_file_name)
                
                # Salvar arquivo individual
                with pd.ExcelWriter(individual_file_path, engine='openpyxl') as writer:
                    df.to_excel(writer, sheet_name='Dados', index=False)
                
                # Thread-safe status reporting
                with status_lock:
                    status_report.append(f"‚úÖ {name}: {len(df)} linhas ‚Üí {individual_file_name}")
                
                logging.info(f"[{thread_name}] ‚úÖ '{name}' salvo como '{individual_file_name}' ({len(df)} linhas)")
                return True, name, len(df), individual_file_name
                
        except Exception as e:
            # Thread-safe error reporting
            with status_lock:
                status_report.append(f"‚ùå {name}: ERRO - {str(e)[:100]}...")
            
            logging.error(f"[{thread_name}] ‚ùå Falha ao executar '{name}': {e}")
            return False, name, 0, None

    try:
        logging.info(f"üöÄ Iniciando execu√ß√£o PARALELA para {len(QUERIES)} queries...")
        logging.info(f"üìÅ Cada query ser√° salva como arquivo individual em: {export_path}")
        
        # Executar queries em paralelo com ThreadPoolExecutor
        successful_queries = 0
        failed_queries = 0
        total_rows = 0
        
        with ThreadPoolExecutor(max_workers=4, thread_name_prefix="QueryWorker") as executor:
            # Submeter todas as queries para execu√ß√£o paralela
            future_to_query = {executor.submit(execute_query_and_save, item): item[0] 
                             for item in QUERIES.items()}
            
            # Processar resultados conforme completam
            for future in as_completed(future_to_query):
                query_name = future_to_query[future]
                try:
                    success, name, rows, filename = future.result()
                    if success:
                        successful_queries += 1
                        total_rows += rows
                    else:
                        failed_queries += 1
                except Exception as e:
                    failed_queries += 1
                    logging.error(f"Erro n√£o capturado para query '{query_name}': {e}")

        logging.info(f"üéâ Processamento paralelo conclu√≠do!")
        logging.info(f"üìä {successful_queries} arquivos criados com sucesso em '{export_path}'")
        logging.info(f"üìà Total de {total_rows:,} linhas de dados processadas")
        if failed_queries > 0:
            logging.warning(f"‚ö†Ô∏è {failed_queries} queries falharam")

        # --- FASE 3: Relat√≥rio Final de Status ---
        # Verificar se todas as queries cr√≠ticas foram executadas com sucesso
        critical_queries = [
            "Notas Fiscais Cadastradas - Compara√ß√£o YoY",
            "Vendas Cadastradas - Compara√ß√£o YoY",
            "Compradores √önicos",
            "Clientes por Categoria"
        ]
        
        all_critical_success = all(name in dataframes_dict for name in critical_queries)
        
        # Contadores de status
        total_queries = len(QUERIES)
        successful_queries = len([s for s in status_report if 'Sucesso' in s])
        failed_queries = total_queries - successful_queries
        
        logging.info("=" * 60)
        logging.info("RELAT√ìRIO DE EXECU√á√ÉO FINAL")
        logging.info("=" * 60)
        logging.info(f"üìÅ Arquivo gerado: {file_name}")
        logging.info(f"üìÇ Localiza√ß√£o: {export_path}")
        logging.info(f"üìä Queries executadas: {successful_queries}/{total_queries}")
        
        if failed_queries > 0:
            logging.warning(f"‚ö†Ô∏è  Queries com falha: {failed_queries}")
        
        if all_critical_success:
            logging.info("‚úÖ Todas as queries cr√≠ticas executadas com sucesso!")
        else:
            missing_queries = [q for q in critical_queries if q not in dataframes_dict]
            logging.warning(f"‚ùå Queries cr√≠ticas faltantes: {missing_queries}")
        
        logging.info("üìã Status detalhado:")
        for name in QUERIES.keys():
            if name in dataframes_dict:
                logging.info(f"  ‚úÖ {name}")
            else:
                logging.error(f"  ‚ùå {name}")
        
        logging.info("=" * 60)

    except Exception as e:
        # Tratamento de erro geral - captura falhas n√£o previstas
        logging.error(f"Ocorreu um erro geral durante a execu√ß√£o do processo: {e}")
        logging.error("Falha cr√≠tica durante a gera√ß√£o do arquivo Excel.")

    logging.info("Processo de extra√ß√£o de relat√≥rios finalizado.")


if __name__ == "__main__":
    """
    Ponto de entrada principal do script.
    
    Quando executado diretamente (n√£o importado como m√≥dulo), inicia o processo
    de gera√ß√£o de relat√≥rios mensais do I-Club.
    
    Formas de Execu√ß√£o:
        - Manual: python main.py
        - Agendada: Via cron ou agendador de tarefas do sistema
        - Automatizada: Integra√ß√£o com ferramentas de automa√ß√£o
        
    Logs de Execu√ß√£o:
        - Arquivo: automation.log (mesmo diret√≥rio do script)
        - Console: Output em tempo real durante execu√ß√£o
        
    Tempo Estimado: 3-4 minutos para execu√ß√£o completa
    """
    run_report_job()